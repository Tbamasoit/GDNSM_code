# trainer.py
import torch
from tqdm import tqdm
import logging
import os
import time
from torch.utils.tensorboard import SummaryWriter # <--- æ ¸å¿ƒç»„ä»¶
from utils.scheduler import DynamicDifficultyScheduler 
from utils.topk_evaluator import TopKEvaluator # å¯¼å…¥æ–°çš„è¯„ä¼°å™¨
from types import SimpleNamespace # <--- æ–°å¢è¿™ä¸ªå¼•ç”¨

logger = logging.getLogger(__name__)

class Trainer:
    def __init__(self, config, model, train_dataloader, valid_dataloader, optimizer):
        self.config = config
        self.model = model
        self.train_dataloader = train_dataloader
        self.valid_dataloader = valid_dataloader
        self.optimizer = optimizer
        self.device = config['device']
        self.evaluator = TopKEvaluator(config)
        self.epoch_diff_loss = 0.0
        self.epoch_rec_loss = 0.0
        # åˆå§‹åŒ–è°ƒåº¦å™¨
        self.scheduler = DynamicDifficultyScheduler(config)
        
        # åˆå§‹åŒ– Diffusion Sampler (å¯¹åº”è®ºæ–‡ 2.4.2 èŠ‚)
        # self.sampler = DiffusionSampler(model.diffusion_model, config)
        # === [æ–°å¢] åˆå§‹åŒ– TensorBoard Writer ===
        # ä¸ºäº†é˜²æ­¢ä¸åŒå®éªŒçš„æ—¥å¿—è¦†ç›–ï¼Œæˆ‘ä»¬ç”¨ "å½“å‰æ—¶é—´" æˆ– "é…ç½®å‚æ•°" åšå­æ–‡ä»¶å¤¹å
        timestamp = time.strftime('%m%d_%H%M')
        exp_name = f"{config['dataset']}_d{config['d_epoch']}_lam{config['lambda_neg']}_{timestamp}"
        log_dir = os.path.join(config['tensorboard_log_dir'], exp_name)
        
        self.writer = SummaryWriter(log_dir=log_dir)
        self.global_step = 0 # ç”¨äºè®°å½•æ€»çš„ Batch æ•°
        
        logger.info(f"TensorBoard initialized. Logs will be saved to: {log_dir}")


    def _train_epoch(self, epoch_idx):
        self.model.train()
        epoch_total_loss = 0.0
        
        # ä½¿ç”¨ tqdm åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡
        for batch_idx, batch_data in enumerate(tqdm(self.train_dataloader, desc=f"Epoch {epoch_idx}")):
            # --- 1. æ•°æ®è§£åŒ…ä¸ç§»åŠ¨ (æ ¹æ®ä¾¦å¯Ÿç¬”è®°ä¿®æ”¹) ---
            # batch_data çš„å½¢çŠ¶æ˜¯ (3, batch_size)
            
            # --- [æ ¸å¿ƒä¿®æ”¹] æ•°æ®è§£åŒ…ä¸ç§»åŠ¨ ---
            # batch_data æ˜¯ä¸€ä¸ªåˆ—è¡¨: [users, pos_items, neg_items]
            # æˆ‘ä»¬ä¸èƒ½ç›´æ¥ batch_data.to(device)ï¼Œå¿…é¡»é€ä¸ªç§»åŠ¨
            users, pos_items, neg_items = batch_data
            
            users = users.to(self.device).long()
            pos_items = pos_items.to(self.device).long()
            neg_items = neg_items.to(self.device).long()

            # é‡æ–°æ‰“åŒ…æˆåˆ—è¡¨ï¼Œä»¥ä¾¿ä¼ ç»™ calculate_loss
            interaction = [users, pos_items, neg_items]

            # batch_data = batch_data.to(self.device) # å…ˆæŠŠæ•´ä¸ªå¼ é‡ç§»åŠ¨åˆ°è®¾å¤‡
        
            # users = batch_data[0, :]
            # pos_items = batch_data[1, :]
            # neg_items = batch_data[2, :]

            # ======================================================================
            # é˜¶æ®µä¸€: è®­ç»ƒæ‰©æ•£æ¨¡å‹ (å¯¹åº” Algorithm 1, lines 7-12)
            # ======================================================================

            # 1. å‡†å¤‡æ•°æ®ï¼šè·å–æ­£æ ·æœ¬åµŒå…¥å’Œæ¡ä»¶ä¿¡æ¯
            # æˆ‘ä»¬éœ€è¦æ¨¡å‹æä¾›ä¸€ä¸ªæ–¹æ³•æ¥è·å–è¿™äº›ä¸œè¥¿
            with torch.no_grad():
                pos_item_embeds, diffusion_conditions = self.model.get_diffusion_inputs(users, pos_items)

            # 2. å¾ªç¯è®­ç»ƒæ‰©æ•£æ¨¡å‹ N æ¬¡ (è¿™é‡Œè®¾ä¸º 3)
            # è¿™èƒ½è®©æ‰©æ•£æ¨¡å‹æ›´å¿«åœ°é€‚åº” Item Embedding çš„å˜åŒ–ï¼Œç”Ÿæˆæ›´ç²¾å‡†çš„æ ·æœ¬
            for _ in range(3):
                self.optimizer.zero_grad()
                diffusion_loss = self.model.diffusion_MM(pos_item_embeds, diffusion_conditions, self.device)
                # diffusion_loss = self.model.diffusion_MM(pos_item_embeds, diffusion_conditions, self.device)
                
                # 3. åå‘ä¼ æ’­å¹¶æ›´æ–° (åªæ›´æ–°æ‰©æ•£æ¨¡å‹çš„å‚æ•°)
                # ä¸ºäº†å®ç°äº¤æ›¿è®­ç»ƒï¼Œç†æƒ³æƒ…å†µä¸‹åº”è¯¥æœ‰ä¸¤ä¸ªä¼˜åŒ–å™¨ã€‚
                # ç®€åŒ–ç‰ˆï¼šæˆ‘ä»¬å…ˆç”¨ä¸€ä¸ªä¼˜åŒ–å™¨æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚
                diffusion_loss.backward()
                
                # (å¯é€‰) å¦‚æœä½ ä¹‹å‰åŠ äº†æ¢¯åº¦è£å‰ªï¼Œè¿™é‡Œä¹Ÿå¯ä»¥åŠ ï¼Œé€šå¸¸æ‰©æ•£æ¨¡å‹æ¯”è¾ƒç¨³ï¼Œä¸åŠ ä¹Ÿè¡Œ
                # torch.nn.utils.clip_grad_norm_(self.model.diffusion_MM.parameters(), max_norm=1.0)
                
                self.optimizer.step()

            # ======================================================================
            # é˜¶æ®µäºŒ: è®­ç»ƒæ¨èæ¨¡å‹ (å¯¹åº” Algorithm 1, lines 13-26)
            # ======================================================================

            # 1. [æ ¸å¿ƒé€»è¾‘] åŠ¨æ€è´Ÿæ ·æœ¬ç”Ÿæˆä¸è°ƒåº¦
            scheduled_negs = None

            # åªæœ‰å½“ Epoch è¾¾åˆ°é˜ˆå€¼ï¼Œä¸” g(epoch) > 0 æ—¶æ‰ç”Ÿæˆ
            # æˆ‘ä»¬å¯ä»¥å…ˆé—®é—® scheduler éœ€è¦å¤šå°‘ä¸ªï¼Œå¦‚æœéœ€è¦0ä¸ªï¼Œå°±åˆ«è´¹åŠ²ç”Ÿæˆäº†ï¼Œçœæ—¶é—´
            required_num = self.scheduler.get_g_epoch(epoch_idx)

            if required_num > 0:
                with torch.no_grad():
                    # è°ƒç”¨æ¨¡å‹ç”Ÿæˆæ¥å£
                    neg_v, neg_t, neg_vt = self.model.generate_batch_negatives(users, pos_items)
                    
                    # è°ƒç”¨è°ƒåº¦å™¨ç­›é€‰
                    scheduled_negs = self.scheduler.schedule(epoch_idx, neg_v, neg_t, neg_vt)

            # 2. è®¡ç®—æ¨è Loss
            self.optimizer.zero_grad()
            # å°†ç­›é€‰åçš„è´Ÿæ ·æœ¬ä¼ å…¥ calculate_loss
            recommender_loss = self.model.calculate_loss(interaction,  generated_negs=scheduled_negs)
            # 3. åå‘ä¼ æ’­ (æ›´æ–°æ¨èæ¨¡å‹)
            recommender_loss.backward()
            # [æ–°å¢] æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢ Loss çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            # --- è®°å½•æ€»æŸå¤± ---
            # total_loss = diffusion_loss + recommender_loss
            # epoch_total_loss += total_loss.item()
            epoch_total_loss += (diffusion_loss.item() + recommender_loss.item())
            # ç®€å•èµ·è§ï¼Œä½ å¯ä»¥ç›´æ¥ print å‡ºæ¥è°ƒè¯•
            if batch_idx % 20 == 0:
                logger.info(f"Batch {batch_idx}: Diff Loss={diffusion_loss.item():.4f}, Rec Loss={recommender_loss.item():.4f}")                                                 
            
            # === [æ–°å¢] è®°å½• Step çº§åˆ«çš„ Loss ===
            # è®°å½•æ‰©æ•£æ¨¡å‹ Loss
            self.writer.add_scalar('Loss/Diffusion_Step', diffusion_loss.item(), self.global_step)
            
            # è®°å½•æ¨èæ¨¡å‹ Loss
            self.writer.add_scalar('Loss/Recommendation_Step', recommender_loss.item(), self.global_step)
            
            # è®°å½•æ€» Loss
            self.writer.add_scalar('Loss/Total_Step', diffusion_loss.item() + recommender_loss.item(), self.global_step)

            # === [è¿›é˜¶] ç›‘æ§ç”Ÿæˆçš„è´Ÿæ ·æœ¬è´¨é‡ (å¼ºçƒˆæ¨è) ===
            # è¿™èƒ½å¸®ä½ ä¸€çœ¼çœ‹å‡ºç”Ÿæˆçš„æ ·æœ¬æ˜¯ä¸æ˜¯å¤ªç®€å•(å¾—åˆ†ä½)æˆ–è€…å¤ªéš¾(å¾—åˆ†é«˜)
            if scheduled_negs is not None:
                with torch.no_grad():
                    # è®¡ç®— User å’Œ Generated Neg çš„ç›¸ä¼¼åº¦
                    # users å½¢çŠ¶ [B], scheduled_negs å½¢çŠ¶ [B, 1, D] -> [B, D]
                    # u_emb éœ€è¦é‡æ–°è·å–ä¸€ä¸‹æˆ–è€…åœ¨ calculate_loss é‡Œè¿”å›ï¼Œè¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€åŒ–ä¸€ä¸‹é€»è¾‘
                    # å¦‚æœä½ æƒ³åšè¿™ä¸ªï¼Œéœ€è¦åœ¨ calculate_loss é‡ŒæŠŠè®¡ç®—å¥½çš„åˆ†æ•°ä¼ å‡ºæ¥ï¼Œæˆ–è€…åœ¨è¿™é‡Œç®€å•ç®—ä¸€ä¸‹
                    pass 

            self.global_step += 1 # æ­¥æ•° +1


            # --- å†’çƒŸæµ‹è¯• (ä¿æŒ) ---
            if batch_idx >= 0 and self.config['smoke_test']:
                print("\nSmoke test passed for one batch!")
                break

        return epoch_total_loss / (batch_idx + 1)



# common/trainer.py

    @torch.no_grad()
    def _valid_epoch(self, epoch_idx):
        """
        å†…éƒ¨æ–¹æ³•ï¼Œè´Ÿè´£åè°ƒå’Œé©±åŠ¨ä¸€è½®å®Œæ•´çš„ã€ä¸¤é˜¶æ®µçš„éªŒè¯ã€‚
        """
        self.model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        
        # --- é˜¶æ®µä¸€ï¼šæ”¶é›†æ‰€æœ‰æ‰¹æ¬¡çš„ Top-K ç»“æœ ---
        
        batch_matrix_list = [] # ç”¨äºå­˜å‚¨æ¯ä¸ªæ‰¹æ¬¡çš„ topk_index
        
        # éå†éªŒè¯æ•°æ®åŠ è½½å™¨
        for batch_idx, batch_data in enumerate(tqdm(self.valid_dataloader, desc=f"Epoch {epoch_idx} | Validation")):
            # å‡è®¾ valid_dataloader æ¯æ¬¡è¿”å›ä¸€ä¸ªæ‰¹æ¬¡çš„ç”¨æˆ·ID
            # users_batch = batch_data.to(self.device)
            users_batch = batch_data[0] # å–åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
            users_batch = users_batch.to(self.device).long()
            
            # 1. è°ƒç”¨æ¨¡å‹ç”Ÿæˆå½“å‰æ‰¹æ¬¡ç”¨æˆ·çš„åˆ†æ•°
            scores_batch = self.model.full_sort_predict([users_batch])
            
            # 2. è°ƒç”¨è¯„ä¼°å™¨çš„ collect æ–¹æ³•å¤„ç†å½“å‰æ‰¹æ¬¡çš„ç»“æœ
            # æ³¨æ„ï¼šç¬¬äºŒä¸ªå‚æ•° interaction å¯ä»¥æš‚æ—¶ç”¨ä¸€ä¸ªç®€å•çš„å¯¹è±¡æˆ– None æ›¿ä»£
            # å› ä¸ºåœ¨ full sort æ¨¡å¼ä¸‹ï¼Œå®ƒå¯èƒ½ä¸æ˜¯å¿…éœ€çš„ã€‚éœ€è¦æ ¹æ®å…·ä½“å®ç°è°ƒæ•´ã€‚
            # è¿™é‡Œæˆ‘ä»¬å‡è®¾å®ƒä¸éœ€è¦ interaction å‚æ•°ã€‚
            # === [æ ¸å¿ƒä¿®å¤] æ„é€  Mock Interaction å¯¹è±¡ ===
            batch_size = users_batch.size(0)
            # æ„é€ ä¸€ä¸ªå¸¦æœ‰ user_len_list å±æ€§çš„å¯¹è±¡
            # åœ¨ Full Sort æ¨¡å¼ä¸‹ï¼Œé€šå¸¸è®¤ä¸ºæ¯ä¸ª batch é‡Œçš„æ¯ä¸ªä½ç½®å¯¹åº” 1 ä¸ªç”¨æˆ·
            interaction = SimpleNamespace(
                user_len_list=[1] * batch_size,
                pos_len_list=[1] * batch_size  #æœ‰äº› evaluator å¯èƒ½ä¹Ÿéœ€è¦è¿™ä¸ªï¼Œé¡ºæ‰‹åŠ ä¸Šä¿é™©
            )
            # 2. æ”¶é›† TopK
            # å°†æ„é€ å¥½çš„ interaction ä¼ è¿›å»ï¼Œè€Œä¸æ˜¯ None
            topk_index_batch = self.evaluator.collect(interaction, scores_batch, full=True)
            
            # 3. å°†å½“å‰æ‰¹æ¬¡çš„ç»“æœå­˜å…¥åˆ—è¡¨
            batch_matrix_list.append(topk_index_batch)

            # å†’çƒŸæµ‹è¯•
            if self.config['smoke_test']:
                break
            
        # --- é˜¶æ®µäºŒï¼šè°ƒç”¨è¯„ä¼°å™¨çš„ evaluate æ–¹æ³•è¿›è¡Œæœ€ç»ˆè®¡ç®— ---
        
        # å°† valid_dataloader è‡ªèº«ä½œä¸º eval_data ä¼ å…¥
        # === [æ ¸å¿ƒä¿®å¤] ä¼ å…¥ dataset è€Œä¸æ˜¯ dataloader ===
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¼ å…¥åŸå§‹çš„ valid_dataloader å¯¹è±¡ï¼Œå› ä¸º evaluator éœ€è¦ç”¨åˆ°å®ƒçš„ ground truth
        eval_data = self.valid_dataloader.dataset  # è·å–åº•å±‚çš„æ•°æ®é›†å¯¹è±¡
        results = self.evaluator.evaluate(batch_matrix_list, eval_data)

        # --- è®°å½•å’Œè¿”å›å…³é”®æŒ‡æ ‡ (é€»è¾‘ä¿æŒä¸å˜) ---
        key_metric = self.config['valid_metric']
        metric_name, k = key_metric.split('@')
        k = int(k)
        # å¤„ç†å¤§å°å†™å…¼å®¹æ€§ (evaluator é€šå¸¸è¿”å›å°å†™ keyï¼Œå¦‚ recall@20)
        result_key = f'{metric_name.lower()}@{k}'
        final_score = results.get(result_key, 0.0) # æ³¨æ„ evaluator è¿”å›çš„æ˜¯å°å†™ key
        # ç®€å•æ‰“å°ä¸€ä¸‹éªŒè¯ç»“æœ
        logger.info(f"Validation Results (Epoch {epoch_idx}): {results}")

        # self._log_metrics(results, epoch_idx, 'valid')
        
        return final_score

    def fit(self):
        """
        Drives the complete training and validation loop, with integrated negative sampling
        at the beginning of each epoch.
        """
        logger.info("================== Starting Training ==================")
        for epoch_idx in range(self.config['epochs']):
            
            # --- [æ ¸å¿ƒä¿®æ”¹] åœ¨æ¯ä¸ª epoch å¼€å§‹å‰ï¼Œæ‰§è¡Œè´Ÿé‡‡æ · ---
            logger.info(f"Epoch {epoch_idx} | Phase: Negative Sampling")
            # æˆ‘ä»¬é€šè¿‡ train_dataloader è®¿é—®å…¶å†…éƒ¨çš„ dataset å¯¹è±¡ (å³ TrnData å®ä¾‹)
            # ç„¶åè°ƒç”¨å®ƒçš„ negSampling æ–¹æ³•
            self.train_dataloader.dataset.negSampling()
            logger.info("Negative sampling for epoch completed.")

            # --- è®­ç»ƒé˜¶æ®µ ---
            train_loss = self._train_epoch(epoch_idx)
            # logger.info(f"Epoch {epoch_idx} | Train Loss: {train_loss:.4f}")
            # self._log_metrics({'train_loss': train_loss}, epoch_idx, 'train') # å‡è®¾æœ‰ log æ–¹æ³•
            # === [æ–°å¢] è®°å½• Epoch çº§åˆ«çš„ Train Loss ===
            self.writer.add_scalar('Loss/Train_Epoch', train_loss, epoch_idx)


            # --- è¯„ä¼°é˜¶æ®µ ---
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾è¯„ä¼°çš„ epoch
            if (epoch_idx + 1) % self.config['eval_step'] == 0:
                # è°ƒç”¨æˆ‘ä»¬å·²ç»å†™å¥½çš„è¯„ä¼°æ–¹æ³•
                valid_score = self._valid_epoch(epoch_idx)
                logger.info(f"Epoch {epoch_idx} | Valid Score ({self.config['valid_metric']}): {valid_score:.4f}")

                # === [æ–°å¢] è®°å½•éªŒè¯é›†æŒ‡æ ‡ ===
                metric_name = self.config['valid_metric'] # e.g. Recall@20
                self.writer.add_scalar(f'Metric/{metric_name}', valid_score, epoch_idx)
                # å¦‚æœä½ æƒ³è®°å½•æ›´å¤šï¼Œæ¯”å¦‚ NDCG@10ï¼Œå¯ä»¥åœ¨ _valid_epoch é‡ŒæŠŠ results è¿”å›å‡ºæ¥
                # self.writer.add_scalar('Metric/NDCG@10', results['ndcg@10'], epoch_idx)

                # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ¨¡å‹ä¿å­˜å’Œæ—©åœçš„é€»è¾‘
                # ...

            # å†’çƒŸæµ‹è¯•ï¼šåªè·‘ä¸€ä¸ª Epoch å°±é€€å‡ºæ•´ä¸ªè®­ç»ƒ
            if self.config['smoke_test']:
                logger.info("ğŸ’¨ Smoke test finished. Stopping training.")
                break
        
        # === [æ–°å¢] è®­ç»ƒç»“æŸå…³é—­ Writer ===
        self.writer.close()
        logger.info("================== Training Finished ==================")

    # def evaluate(self):
    #     # ... è¯„ä¼°é€»è¾‘ ...
    #     pass
